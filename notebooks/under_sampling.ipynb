{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "import scipy\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from hypopt import GridSearch\n",
    "from hypopt.model_selection import GridSearch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get vocab and numer_stats files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/mm-cpc-generator/train/categorical-vocab.json') as f:\n",
    "    cat_vocab = json.load(f)\n",
    "f.close()\n",
    "\n",
    "with open(\"../data/processed/mm-cpc-generator/train/numerical-stats.json\") as f:\n",
    "    numer_stats = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_key_len_mapper(cat_vocab):\n",
    "    idx_mapper = {}\n",
    "    for key, value in cat_vocab.items():\n",
    "        temp_idx = {}\n",
    "        for i, val in enumerate(value):\n",
    "            temp_idx[val] = i\n",
    "        idx_mapper[key] = temp_idx\n",
    "    return idx_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_array(cat_vocab, numer_stats, data_df, data_type=\"csv\"):\n",
    "    if data_type == \"csv\":\n",
    "        data_df = pd.read_csv(data_df)\n",
    "    idx_mapper = get_vocab_key_len_mapper(cat_vocab)\n",
    "    count = 0\n",
    "    data_df = data_df.drop([\"conversion_target\"], axis=1)\n",
    "    for col in numer_stats:\n",
    "        if numer_stats[col]['std'] == 0:\n",
    "            continue\n",
    "        data_df[col] = (data_df[col] - numer_stats[col]['mean'])/numer_stats[col]['std']\n",
    "    df_cols = list(data_df.columns)\n",
    "    for idx, row in data_df.iterrows():\n",
    "        row_list = []\n",
    "        for col in df_cols:\n",
    "            if col in idx_mapper:\n",
    "                temp = [0]*(len(idx_mapper[col]) + 1)\n",
    "                if row[col] in idx_mapper[col]:\n",
    "                    temp[idx_mapper[col][row[col]]] = 1\n",
    "                else:\n",
    "                    temp[-1] = 1\n",
    "            else:\n",
    "                temp = [row[col]]\n",
    "            row_list += temp\n",
    "        if idx == 0:\n",
    "            pos_arr = coo_matrix(row_list)\n",
    "        else:\n",
    "            temp_sparse = coo_matrix(row_list)\n",
    "            pos_arr = vstack([pos_arr, temp_sparse])\n",
    "        count += 1\n",
    "        if count%10000 == 0:\n",
    "            print(\"finished \", count, \"samples\")\n",
    "    return pos_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create sparse array for positive training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/processed/mm-cpc-generator/train/\"\n",
    "files = os.listdir(dir_path)\n",
    "flag = True\n",
    "for i,file in enumerate(files):\n",
    "    if \"positive\" in file:\n",
    "        filepath = dir_path + file\n",
    "        if flag:\n",
    "            pos_sparse_arr = get_sparse_array(cat_vocab, numer_stats,filepath)\n",
    "            flag = False\n",
    "        else:\n",
    "            temp_arr = get_sparse_array(cat_vocab, numer_stats,filepath)\n",
    "            pos_sparse_arr = vstack([pos_sparse_arr, temp_arr])\n",
    "\n",
    "scipy.sparse.save_npz('../data/intermediate/pos_sparse_arr',pos_sparse_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sparse array for negative training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnegativearray(cat_vocab, numer_stats, dir_path, count):\n",
    "    files = os.listdir(dir_path)\n",
    "    flag = True\n",
    "    counter = 0\n",
    "    files = [d for d in files if \"negative\" in d]\n",
    "    random.shuffle(files)\n",
    "    for i,file in enumerate(files):\n",
    "        if counter == count:\n",
    "            break\n",
    "        filepath = dir_path + file\n",
    "        if flag:\n",
    "            sparse_arr = get_sparse_array(cat_vocab, numer_stats, filepath)\n",
    "            flag = False\n",
    "        else:\n",
    "            temp_arr = get_sparse_array(cat_vocab, numer_stats, filepath)\n",
    "            sparse_arr = vstack([sparse_arr, temp_arr])\n",
    "        counter += 1\n",
    "    return sparse_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/processed/mm-cpc-generator/train/\"\n",
    "neg_sparse_arr = getnegativearray(cat_vocab, numer_stats, dir_path, count=14)\n",
    "scipy.sparse.save_npz('../data/intermediate/neg_sparse_arr',neg_sparse_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/processed/mm-cpc-generator/validation/part-00000-f7421232-0a5f-43a1-a30d-359dd1e1b618-c000.csv\"\n",
    "valid_df = pd.read_csv(filepath)\n",
    "y_valid = valid_df.conversion_target.values\n",
    "X_valid = get_sparse_array(cat_vocab, numer_stats, valid_df, data_type=\"pandas\")\n",
    "scipy.sparse.save_npz('../data/intermediate/valid_sparse_arr', X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/processed/mm-cpc-generator/test/part-00000-66d695ad-776c-44b9-8feb-9f3906f297a0-c000.csv\"\n",
    "test_df = pd.read_csv(filepath)\n",
    "y_test = test_df.conversion_target.values\n",
    "X_test = get_sparse_array(cat_vocab, numer_stats, test_df, data_type=\"pandas\")\n",
    "scipy.sparse.save_npz('../data/intermediate/test_sparse_arr',X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pos = scipy.sparse.load_npz('../data/intermediate/pos_sparse_arr.npz')\n",
    "X_neg = scipy.sparse.load_npz('../data/intermediate/full_neg_sparse_arr.npz')\n",
    "X_train = vstack([X_pos, X_neg])\n",
    "y_pos = [1]*X_pos.shape[0]\n",
    "y_neg = [0]*X_neg.shape[0]\n",
    "y_train = y_pos + y_neg\n",
    "y_train = np.array(y_train)\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/processed/mm-cpc-generator/validation/part-00000-f7421232-0a5f-43a1-a30d-359dd1e1b618-c000.csv\"\n",
    "valid_df = pd.read_csv(filepath)\n",
    "y_valid = valid_df.conversion_target.values\n",
    "X_valid = scipy.sparse.load_npz('../data/intermediate/valid_sparse_arr.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../data/processed/mm-cpc-generator/test/part-00000-66d695ad-776c-44b9-8feb-9f3906f297a0-c000.csv\"\n",
    "test_df = pd.read_csv(filepath)\n",
    "y_test = test_df.conversion_target.values\n",
    "X_test = scipy.sparse.load_npz('../data/intermediate/test_sparse_arr.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training roc auc score is:  1.0\n",
      "validation roc auc score is:  0.8456836796445666\n",
      "test roc auc score is:  0.9349833819723681\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf.predict_proba(X_train)\n",
    "y_valid_pred = clf.predict_proba(X_valid)\n",
    "y_test_pred = clf.predict_proba(X_test)\n",
    "train_auc_score = roc_auc_score(y_train, y_train_pred.T[1])\n",
    "valid_auc_score = roc_auc_score(y_valid, y_valid_pred.T[0])\n",
    "test_auc_score = roc_auc_score(y_test, y_test_pred.T[0])\n",
    "print(\"training roc auc score is: \", train_auc_score)\n",
    "print(\"validation roc auc score is: \", valid_auc_score)\n",
    "print(\"test roc auc score is: \", test_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall and F1 score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88689899, 0.08928571]),\n",
       " array([9.99797873e-01, 1.55400155e-04]),\n",
       " array([9.39970526e-01, 3.10260308e-04]),\n",
       " array([252317,  32175]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_dis = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_test_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 30 parameter setting(s) using 30 CPU thread(s) ( 1 job(s) per thread ).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [{'C':np.linspace(0.001, 100, num=30),'penalty':['l2']}]\n",
    "gs = GridSearch(model=LogisticRegression())\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "gs.fit(X_train, y_train, param_grid, X_valid, y_valid, scoring=scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training roc auc score is:  1.0\n",
      "validation roc auc score is:  0.8227894296169733\n",
      "test roc auc score is:  0.9034792923797628\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = gs.predict_proba(X_train)\n",
    "y_valid_pred = gs.predict_proba(X_valid)\n",
    "y_test_pred = gs.predict_proba(X_test)\n",
    "train_auc_score = roc_auc_score(y_train, y_train_pred.T[1])\n",
    "valid_auc_score = roc_auc_score(y_valid, y_valid_pred.T[0])\n",
    "test_auc_score = roc_auc_score(y_test, y_test_pred.T[0])\n",
    "print(\"training roc auc score is: \", train_auc_score)\n",
    "print(\"validation roc auc score is: \", valid_auc_score)\n",
    "print(\"test roc auc score is: \", test_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88689859, 0.0877193 ]),\n",
       " array([9.99793910e-01, 1.55400155e-04]),\n",
       " array([9.39968552e-01, 3.10250683e-04]),\n",
       " array([252317,  32175]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_dis = gs.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_test_dis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Validation and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vstack([X_train, X_valid])\n",
    "y_train = np.concatenate([y_train, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training roc auc score is:  0.9993892079471345\n",
      "validation roc auc score is:  0.9870191685449049\n",
      "test roc auc score is:  0.9402191538394806\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf.predict_proba(X_train)\n",
    "y_valid_pred = clf.predict_proba(X_valid)\n",
    "y_test_pred = clf.predict_proba(X_test)\n",
    "train_auc_score = roc_auc_score(y_train, y_train_pred.T[1])\n",
    "valid_auc_score = roc_auc_score(y_valid, y_valid_pred.T[1])\n",
    "test_auc_score = roc_auc_score(y_test, y_test_pred.T[1])\n",
    "print(\"training roc auc score is: \", train_auc_score)\n",
    "print(\"validation roc auc score is: \", valid_auc_score)\n",
    "print(\"test roc auc score is: \", test_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88691596, 0.22580645]),\n",
       " array([9.99904882e-01, 2.17560218e-04]),\n",
       " array([9.40027348e-01, 4.34701608e-04]),\n",
       " array([252317,  32175]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_dis = clf.predict(X_test)\n",
    "precision_recall_fscore_support(y_test, y_test_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pavan",
   "language": "python",
   "name": "pavan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
