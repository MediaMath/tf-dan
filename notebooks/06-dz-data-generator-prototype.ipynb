{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:07:24.919375Z",
     "start_time": "2018-11-22T06:07:22.482187Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/derekzhao/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# tf.contrib.data.CsvDataset can only read GZIP'ed CSV's in version 1.11\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:07:55.878078Z",
     "start_time": "2018-11-22T06:07:55.868904Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/processed/mm-cpc-generator/train/'\n",
    "BATCH_SIZE = 1000\n",
    "\n",
    "DROP_COLS = ['column_weights']\n",
    "TARGET_COL = 'conversion_target'\n",
    "\n",
    "csv_paths = glob.glob(os.path.join(DATA_PATH, '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:07:57.367947Z",
     "start_time": "2018-11-22T06:07:57.322999Z"
    }
   },
   "outputs": [],
   "source": [
    "PROCESSED_DIR = '../data/processed/mm-cpc-generator/'\n",
    "with open(\n",
    "  os.path.join(PROCESSED_DIR, 'train', 'categorical-vocab.json'), 'rt') as f:\n",
    "  vocab_map = json.load(f)\n",
    "with open(\n",
    "  os.path.join(PROCESSED_DIR, 'train', 'numerical-stats.json'), 'rt') as f:\n",
    "  stats_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:10:20.851118Z",
     "start_time": "2018-11-22T06:10:20.246351Z"
    },
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "  \n",
    "  def __init__(\n",
    "    self, csv_paths, vocab_map, stats_map, \n",
    "    batch_size=128, target='conversion_target', drop=['column_weights'],\n",
    "    buffer_size=100000):\n",
    "    \"\"\"\n",
    "    Generator that streams data from a set of CSVs and performs in-memory\n",
    "    processing in batches, including one-hot encoding for categorical\n",
    "    features and standardization for numerical features.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    csv_paths : list of str\n",
    "      Paths to each CSV that comprises the dataset. All CSVs must\n",
    "      have the same header.\n",
    "    vocab_map : dict\n",
    "    stats_map : dict\n",
    "    batch_size : int\n",
    "      Number of samples in each batch.\n",
    "    target : str\n",
    "      Name of feature to be treated as prediction target.\n",
    "    drop : list of str\n",
    "      Names of features to be dropped from the dataset.\n",
    "    buffer_size : int\n",
    "      Window for randomly sampling data when shuffling dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    self.csv_paths = csv_paths\n",
    "    self.target = target\n",
    "    self.drop = drop\n",
    "    self.vocab_map = vocab_map\n",
    "    self.stats_map = stats_map\n",
    "    print('Number of categorical columns:', len(vocab_map))\n",
    "    print('Number of numerical columns:', len(stats_map))\n",
    "    \n",
    "    self.header = self._get_header(csv_paths)\n",
    "    self.batch_size = batch_size\n",
    "    self.num_samples = self._get_num_samples(csv_paths)\n",
    "    print('Number of samples:', self.num_samples)\n",
    "    \n",
    "    record_defaults = self._get_record_defaults(\n",
    "      self.header, vocab_map.keys(), stats_map.keys())\n",
    "    self.record_defaults = record_defaults\n",
    "    \n",
    "    self.csv_dataset = tf.contrib.data.CsvDataset(\n",
    "      filenames=self.csv_paths,\n",
    "      header=True,\n",
    "      record_defaults=record_defaults)\n",
    "    \n",
    "    self.iterator = self.csv_dataset\\\n",
    "      .repeat(-1)\\\n",
    "      .shuffle(buffer_size)\\\n",
    "      .batch(batch_size)\\\n",
    "      .make_one_shot_iterator()\n",
    "    \n",
    "    # Convert to list to pop individual tensors later\n",
    "    self.raw_batch = list(self.iterator.get_next())\n",
    "    \n",
    "    self.sess = tf.Session()\n",
    "    self._create_feature_columns()\n",
    "    self._create_transformation()\n",
    "    \n",
    "  @staticmethod\n",
    "  def _get_header(csv_paths):\n",
    "    \"\"\"\n",
    "    Checks that all CSV headers are the same and returns\n",
    "    the common header.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    csv_paths : str\n",
    "      Path to all CSVs that comprise the dataset.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    header : list of str\n",
    "      Names of each column as they appear in the dataset from\n",
    "      left to right.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Checking headers for {} CSVs...'.format(len(csv_paths)))\n",
    "    header_prev = None\n",
    "    for csv_path in csv_paths:\n",
    "      with open(csv_path, 'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        if header_prev and header_prev != header:\n",
    "          raise ValueError('CSV headers are not consistent.')\n",
    "        header_prev = header\n",
    "        \n",
    "    print('Checking if headers consistent with maps...')\n",
    "    categorical_columns = list(vocab_map.keys())\n",
    "    numerical_columns = list(stats_map.keys())\n",
    "    all_columns = sorted(categorical_columns + numerical_columns)\n",
    "    header_sorted = sorted(header)\n",
    "    if all_columns != header_sorted:\n",
    "      raise ValueError('Vocab and stats maps not consistent with header.')\n",
    "    return header\n",
    "  \n",
    "  @staticmethod\n",
    "  def _get_num_samples(csv_paths):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    csv_paths : list of str\n",
    "      Paths to each CSV that comprises the dataset.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    num_samples : int\n",
    "      The number of samples in the dataset.\n",
    "    \"\"\"\n",
    "    num_samples = 0\n",
    "    for csv_path in csv_paths:\n",
    "      with open(csv_path, 'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        num_samples += sum(1 for row in reader) - 1 # Subtract 1 for header\n",
    "    return num_samples\n",
    "  \n",
    "  @staticmethod\n",
    "  def _get_record_defaults(header, categorical_columns, numerical_columns):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    header : list of str\n",
    "      Names of all features in dataset (as they appear from left to right).\n",
    "    categorical_columns : list of str\n",
    "      Names of all categorical features in dataset.\n",
    "    numerical_columns : list of str\n",
    "      Names of all continuous features in dataset.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    record_defaults : list of constant tensors, size = number of columns\n",
    "      List of constant tensors, one tensor for each column, with which to\n",
    "      impute each column.\n",
    "    \"\"\"\n",
    "    if len(header) != len(categorical_columns) + len(numerical_columns):\n",
    "      raise ValueError(\n",
    "        'Categorical and numerical column names not consistent with header.')\n",
    "    \n",
    "    # Map each column name to a constant tensor for imputation\n",
    "    categorical_dict = dict.fromkeys(\n",
    "      categorical_columns, tf.constant(['-1'], dtype=tf.string))\n",
    "    numerical_dict = dict.fromkeys(\n",
    "      numerical_columns, tf.constant([0], dtype=tf.float32))\n",
    "    dtype_map = {**categorical_dict, **numerical_dict}\n",
    "    \n",
    "    # Create list of imputation tensors consistent with order of\n",
    "    # column names in the header\n",
    "    record_defaults = [dtype_map[column] for column in header]\n",
    "    return record_defaults\n",
    "    \n",
    "  def _create_feature_columns(self):\n",
    "    \"\"\"\n",
    "    Creates a list of configured tf.feature_column objects.\n",
    "    \"\"\"\n",
    "    feature_columns = list()\n",
    "    cat_column = tf.feature_column.categorical_column_with_vocabulary_list\n",
    "    for column, vocab in self.vocab_map.items():\n",
    "      vocab = [str(element) for element in vocab]\n",
    "      if column != self.target and column not in self.drop:\n",
    "        if len(vocab) != len(set(vocab)):\n",
    "          diff = len(vocab) - len(set(vocab))\n",
    "          print(\n",
    "            'Warning: {} duplicate terms found and removed from '\n",
    "            'column {}'.format(diff, column))\n",
    "        feature_column = cat_column(\n",
    "          key=column, vocabulary_list=set(vocab), num_oov_buckets=1)\n",
    "        feature_column = tf.feature_column.indicator_column(feature_column)\n",
    "        feature_columns.append(feature_column)\n",
    "      elif column == self.target:\n",
    "        target_column = cat_column(\n",
    "          key=column, vocabulary_list=vocab, num_oov_buckets=0)\n",
    "        self.target_column = tf.feature_column.indicator_column(target_column)\n",
    "      \n",
    "    num_column = tf.feature_column.numeric_column\n",
    "    for column, stats in self.stats_map.items():\n",
    "      if column in self.drop:\n",
    "        continue\n",
    "      standardize = lambda x: (x - stats['mean']) / stats['std']\n",
    "      feature_column = num_column(key=column, normalizer_fn=standardize)\n",
    "      feature_columns.append(feature_column)\n",
    "    self.feature_columns = feature_columns\n",
    "    \n",
    "  def _create_transformation(self):\n",
    "    \"\"\"\n",
    "    Defines how a tensor of raw data is transformed into a tensor of\n",
    "    processed data and initializes relevant variables.\n",
    "    \"\"\"\n",
    "    # Remove the target from the header and raw_batch tensors\n",
    "    target_index = self.header.index(self.target)\n",
    "    self.header.remove(self.target)\n",
    "    raw_batch_y = self.raw_batch.pop(target_index)\n",
    "    \n",
    "    # Remove drop columns from header and raw_batch tensors\n",
    "    for column in self.drop:\n",
    "      drop_index = self.header.index(column)\n",
    "      self.header.remove(column)\n",
    "      self.raw_batch.pop(drop_index)\n",
    "    \n",
    "    # Dictionaries with key=<feature name>, value=<raw_batch_tensor>\n",
    "    features_x = dict(zip(self.header, self.raw_batch))\n",
    "    features_y = {self.target: raw_batch_y}\n",
    "    \n",
    "    # Final tensors of processed data to be run\n",
    "    # by the class' internal Tensorflow session\n",
    "    self.x_processed = tf.feature_column.input_layer(\n",
    "      features_x, self.feature_columns)\n",
    "    self.y_processed = tf.feature_column.input_layer(\n",
    "      features_y, self.target_column)\n",
    "\n",
    "    self.sess.run(tf.global_variables_initializer())\n",
    "    self.sess.run(tf.tables_initializer())\n",
    "\n",
    "  def __next__(self):\n",
    "    \"\"\"\n",
    "    Returns a batch of processed data as a tuple with form \n",
    "    (batch_x, batch_y).\n",
    "    \"\"\"\n",
    "    return self.sess.run([self.x_processed, self.y_processed])\n",
    "  \n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    Returns the number of batches in the dataset\n",
    "    (i.e. number of iterations per epoch).\n",
    "    \"\"\"\n",
    "    return int(np.ceil(self.num_samples / self.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:10:31.741392Z",
     "start_time": "2018-11-22T06:10:21.594820Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categorical columns: 38\n",
      "Number of numerical columns: 59\n",
      "Checking headers for 2 CSVs...\n",
      "Checking if headers consistent with maps...\n",
      "Number of samples: 8018\n",
      "Warning: 1 duplicate terms found and removed from column browser\n",
      "Warning: 1 duplicate terms found and removed from column browser_version\n",
      "Warning: 1 duplicate terms found and removed from column os\n",
      "Warning: 1 duplicate terms found and removed from column os_version\n"
     ]
    }
   ],
   "source": [
    "datagen = DataGenerator(\n",
    "  csv_paths[:2],\n",
    "  vocab_map,\n",
    "  stats_map,\n",
    "  batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:11:02.316141Z",
     "start_time": "2018-11-22T06:11:02.240944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 118861)\n",
      "(64, 2)\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = next(datagen)\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-22T06:11:04.006749Z",
     "start_time": "2018-11-22T06:11:03.999435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- unit tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
